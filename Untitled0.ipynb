{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "137mhpYA9wfwotgf8VkfrP4en2AZvwnco",
      "authorship_tag": "ABX9TyP2yfFlOdisd3tRtXQ+8MYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaneshInnovator/Human-Detection-from-Image-Using-ML/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JEJzRQN9vGL"
      },
      "outputs": [],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "3siDAsQ298No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NzAahI3V64hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the fruit dataset directory\n",
        "data_dir = '/content/drive/MyDrive/Datasets'"
      ],
      "metadata": {
        "id": "EC8mOR9z-C0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the image size and batch size\n",
        "image_size = (100, 100)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "jnnwIo4N6Zur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation to improve model generalization\n",
        "data_generator = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "id": "AWdcwcWq6pS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using the data generator\n",
        "train_data = data_generator.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',  # Use binary classification\n",
        "    subset='training'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Q4TNL07AYN",
        "outputId": "e53454ed-b780-4ac7-de31-33a1f5530668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 690 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = data_generator.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',  # Use binary classification\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGMDGlza7D1o",
        "outputId": "14e37a94-f470-49da-d37f-b707faafb9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 172 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n"
      ],
      "metadata": {
        "id": "c2UDBlvM7MBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=20,\n",
        "    validation_data=validation_data\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTBtdicg7TK7",
        "outputId": "9cd7b4b4-5f21-4ee3-cf11-de96d0d22edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "22/22 [==============================] - 189s 8s/step - loss: 0.6927 - accuracy: 0.5652 - val_loss: 0.6614 - val_accuracy: 0.5814\n",
            "Epoch 2/20\n",
            "22/22 [==============================] - 35s 2s/step - loss: 0.6687 - accuracy: 0.6043 - val_loss: 0.6617 - val_accuracy: 0.6105\n",
            "Epoch 3/20\n",
            "22/22 [==============================] - 35s 2s/step - loss: 0.6594 - accuracy: 0.6362 - val_loss: 0.6315 - val_accuracy: 0.6337\n",
            "Epoch 4/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.6477 - accuracy: 0.6594 - val_loss: 0.6313 - val_accuracy: 0.6860\n",
            "Epoch 5/20\n",
            "22/22 [==============================] - 35s 2s/step - loss: 0.6463 - accuracy: 0.6522 - val_loss: 0.6193 - val_accuracy: 0.6802\n",
            "Epoch 6/20\n",
            "22/22 [==============================] - 33s 2s/step - loss: 0.6121 - accuracy: 0.6710 - val_loss: 0.5739 - val_accuracy: 0.6860\n",
            "Epoch 7/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5686 - accuracy: 0.6913 - val_loss: 0.6352 - val_accuracy: 0.6105\n",
            "Epoch 8/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5571 - accuracy: 0.7087 - val_loss: 0.5723 - val_accuracy: 0.6919\n",
            "Epoch 9/20\n",
            "22/22 [==============================] - 34s 2s/step - loss: 0.5511 - accuracy: 0.7072 - val_loss: 0.6108 - val_accuracy: 0.6453\n",
            "Epoch 10/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5793 - accuracy: 0.6841 - val_loss: 0.5740 - val_accuracy: 0.6628\n",
            "Epoch 11/20\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.5288 - accuracy: 0.7304 - val_loss: 0.6051 - val_accuracy: 0.6047\n",
            "Epoch 12/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5505 - accuracy: 0.7261 - val_loss: 0.5615 - val_accuracy: 0.6860\n",
            "Epoch 13/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5298 - accuracy: 0.7130 - val_loss: 0.5739 - val_accuracy: 0.6570\n",
            "Epoch 14/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5160 - accuracy: 0.7435 - val_loss: 0.5455 - val_accuracy: 0.6977\n",
            "Epoch 15/20\n",
            "22/22 [==============================] - 33s 2s/step - loss: 0.5237 - accuracy: 0.7507 - val_loss: 0.6112 - val_accuracy: 0.6686\n",
            "Epoch 16/20\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.5432 - accuracy: 0.7290 - val_loss: 0.5924 - val_accuracy: 0.6802\n",
            "Epoch 17/20\n",
            "22/22 [==============================] - 33s 2s/step - loss: 0.5223 - accuracy: 0.7536 - val_loss: 0.5614 - val_accuracy: 0.6977\n",
            "Epoch 18/20\n",
            "22/22 [==============================] - 34s 2s/step - loss: 0.5213 - accuracy: 0.7203 - val_loss: 0.6102 - val_accuracy: 0.6047\n",
            "Epoch 19/20\n",
            "22/22 [==============================] - 34s 2s/step - loss: 0.4867 - accuracy: 0.7609 - val_loss: 0.6033 - val_accuracy: 0.6802\n",
            "Epoch 20/20\n",
            "22/22 [==============================] - 33s 2s/step - loss: 0.5019 - accuracy: 0.7565 - val_loss: 0.5286 - val_accuracy: 0.7035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Datasets/human_model.h5')"
      ],
      "metadata": {
        "id": "W31_67r_LtYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an unknown image to perform detection\n",
        "unknown_image = tf.keras.preprocessing.image.load_img('/content/drive/MyDrive/Sample images/download (10).jpeg', target_size=image_size)\n",
        "unknown_image = tf.keras.preprocessing.image.img_to_array(unknown_image)\n",
        "unknown_image = np.expand_dims(unknown_image, axis=0) / 255.0"
      ],
      "metadata": {
        "id": "hSj3sIi---Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction on the unknown image\n",
        "prediction = model.predict(unknown_image)[0]\n",
        "class_label = 'Human' if prediction >= 0.5 else 'Not Human'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWOcVEWAR7L",
        "outputId": "6089a538-6d31-411a-8991-9373195ff0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Image classified as:\", class_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJJaJ5ikAd0N",
        "outputId": "7074ae22-9f59-4f75-edb6-d90f4f8e29c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image classified as: Human\n"
          ]
        }
      ]
    }
  ]
}